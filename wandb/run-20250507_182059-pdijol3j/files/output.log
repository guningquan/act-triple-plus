Found 80 hdf5 files


Data from: ['/home/robot/Dataset_and_Checkpoint/dataset/zip_tie_random']
- Train on [79] episodes
- Test on [1] episodes


Found 80 hdf5 files
Norm stats from: ['/home/robot/Dataset_and_Checkpoint/dataset/zip_tie_random']
Initializing transformations
Initializing transformations
Augment images: False, train_num_workers: 2, val_num_workers: 2
/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
camera_name cam_left_wrist
camera_name cam_right_wrist
Use VQ: False, None, None
******************************************************************************************************************************************************
number of parameters: 117.43M
KL Weight 10
  0%|                                                                                                                                                                  | 0/100001 [00:01<?, ?it/s]
Validating
Traceback (most recent call last):
  File "imitate_episodes_multi_gpu.py", line 904, in <module>
    main(vars(parser.parse_args()))
  File "imitate_episodes_multi_gpu.py", line 233, in main
    best_ckpt_info = train_bc(train_dataloader, val_dataloader, config, policy)
  File "imitate_episodes_multi_gpu.py", line 797, in train_bc
    forward_dict = forward_pass(data, policy)
  File "imitate_episodes_multi_gpu.py", line 744, in forward_pass
    return policy(qpos_data, image_data, action_data, is_pad) # TODO remove None
  File "/home/robot/Programs_Codes/aloha_related/act-triple-plus/policy.py", line 343, in __call__
    a_hat, is_pad_hat, (mu, logvar), probs, binaries = self.model(qpos, image, env_state, actions, is_pad, vq_sample)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/robot/Programs_Codes/aloha_related/act-triple-plus/detr/models/detr_vae.py", line 164, in forward
    features, pos = self.backbones[cam_id](image[:, cam_id])
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/robot/Programs_Codes/aloha_related/act-triple-plus/detr/models/backbone.py", line 151, in forward
    xs = self[0](tensor_list)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/robot/Programs_Codes/aloha_related/act-triple-plus/detr/models/backbone.py", line 224, in forward
    pred = feature_readout(img_rescaled, feature_type, device='cpu')  # [B, 3, H', W']
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/visualpriors/transforms.py", line 58, in feature_readout
    return VisualPrior.to_predicted_label(img, feature_tasks=[feature_task], device=device)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/visualpriors/transforms.py", line 148, in to_predicted_label
    return torch.cat([net(img) for net in nets], dim=1)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/visualpriors/transforms.py", line 148, in <listcomp>
    return torch.cat([net(img) for net in nets], dim=1)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/visualpriors/taskonomy_network.py", line 160, in forward
    return self.decoder(self.encoder(x))
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/visualpriors/taskonomy_network.py", line 392, in forward
    x = self.conv1(x)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
Traceback (most recent call last):
  File "imitate_episodes_multi_gpu.py", line 904, in <module>
    main(vars(parser.parse_args()))
  File "imitate_episodes_multi_gpu.py", line 233, in main
    best_ckpt_info = train_bc(train_dataloader, val_dataloader, config, policy)
  File "imitate_episodes_multi_gpu.py", line 797, in train_bc
    forward_dict = forward_pass(data, policy)
  File "imitate_episodes_multi_gpu.py", line 744, in forward_pass
    return policy(qpos_data, image_data, action_data, is_pad) # TODO remove None
  File "/home/robot/Programs_Codes/aloha_related/act-triple-plus/policy.py", line 343, in __call__
    a_hat, is_pad_hat, (mu, logvar), probs, binaries = self.model(qpos, image, env_state, actions, is_pad, vq_sample)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/robot/Programs_Codes/aloha_related/act-triple-plus/detr/models/detr_vae.py", line 164, in forward
    features, pos = self.backbones[cam_id](image[:, cam_id])
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/robot/Programs_Codes/aloha_related/act-triple-plus/detr/models/backbone.py", line 151, in forward
    xs = self[0](tensor_list)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/robot/Programs_Codes/aloha_related/act-triple-plus/detr/models/backbone.py", line 224, in forward
    pred = feature_readout(img_rescaled, feature_type, device='cpu')  # [B, 3, H', W']
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/visualpriors/transforms.py", line 58, in feature_readout
    return VisualPrior.to_predicted_label(img, feature_tasks=[feature_task], device=device)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/visualpriors/transforms.py", line 148, in to_predicted_label
    return torch.cat([net(img) for net in nets], dim=1)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/visualpriors/transforms.py", line 148, in <listcomp>
    return torch.cat([net(img) for net in nets], dim=1)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/visualpriors/taskonomy_network.py", line 160, in forward
    return self.decoder(self.encoder(x))
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/visualpriors/taskonomy_network.py", line 392, in forward
    x = self.conv1(x)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/ubuntu20/miniforge3/envs/aloha/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same
